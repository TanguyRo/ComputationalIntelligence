-------------Test n°1-------------
Test result without cross validation 80% dataset for training and 20% for test:

random forest {'criterion': 'entropy'}
              precision    recall  f1-score   support

           0       0.93      0.97      0.95       273
           1       0.77      0.57      0.66        47

    accuracy                           0.91       320
   macro avg       0.85      0.77      0.80       320
weighted avg       0.91      0.91      0.91       320

random forest {'criterion': 'gini'}
              precision    recall  f1-score   support

           0       0.92      0.97      0.94       273
           1       0.72      0.49      0.58        47

    accuracy                           0.90       320
   macro avg       0.82      0.73      0.76       320
weighted avg       0.89      0.90      0.89       320

k neighbors {'n_neighbors': 3}
              precision    recall  f1-score   support

           0       0.89      0.94      0.91       273
           1       0.48      0.34      0.40        47

    accuracy                           0.85       320
   macro avg       0.69      0.64      0.66       320
weighted avg       0.83      0.85      0.84       320

k neighbors {'n_neighbors': 5}
              precision    recall  f1-score   support

           0       0.88      0.97      0.92       273
           1       0.53      0.21      0.30        47

    accuracy                           0.86       320
   macro avg       0.70      0.59      0.61       320
weighted avg       0.83      0.86      0.83       320

support vector machines {'C': 0.9}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 1}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 1.1}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 1.2}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 1.3}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 1.4}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'kernel': 'rbf'}
              precision    recall  f1-score   support

           0       0.86      1.00      0.92       273
           1       1.00      0.02      0.04        47

    accuracy                           0.86       320
   macro avg       0.93      0.51      0.48       320
weighted avg       0.88      0.86      0.79       320

support vector machines {'C': 0.1}
              precision    recall  f1-score   support

           0       0.85      1.00      0.92       273
           1       0.00      0.00      0.00        47

    accuracy                           0.85       320
   macro avg       0.43      0.50      0.46       320
weighted avg       0.73      0.85      0.79       320

support vector machines {'C': 0.8}
              precision    recall  f1-score   support

           0       0.85      1.00      0.92       273
           1       0.00      0.00      0.00        47

    accuracy                           0.85       320
   macro avg       0.43      0.50      0.46       320
weighted avg       0.73      0.85      0.79       320

support vector machines {'kernel': 'linear'}
              precision    recall  f1-score   support

           0       0.85      1.00      0.92       273
           1       0.00      0.00      0.00        47

    accuracy                           0.85       320
   macro avg       0.43      0.50      0.46       320
weighted avg       0.73      0.85      0.79       320

Conclusion
Random Forest is best classifier.
However, SVM can't classify good wine.It's because the data is unbalanced (not enough good wine data).

-------------Test n°2-------------
Test results for 10 fold cross validation:

random forest {'criterion': 'gini'}
mean accuracy score: 0.916
mean recall score: 0.55
mean f1 score: 0.604
 
random forest {'criterion': 'entropy'}
mean accuracy score: 0.914
mean recall score: 0.525
mean f1 score: 0.628
 
k neighbors {'n_neighbors': 3}
mean accuracy score: 0.871
mean recall score: 0.419
mean f1 score: 0.462
 
k neighbors {'n_neighbors': 5}
mean accuracy score: 0.861
mean recall score: 0.323
mean f1 score: 0.382
 
support vector machines {'C': 0.1}
mean accuracy score: 0.864
mean recall score: 0.0
mean f1 score: 0.0
 
support vector machines {'C': 0.8}
mean accuracy score: 0.864
mean recall score: 0.0
mean f1 score: 0.0
 
support vector machines {'C': 0.9}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'C': 1}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'C': 1.1}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'C': 1.2}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'C': 1.3}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'C': 1.4}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02
 
support vector machines {'kernel': 'linear'}
mean accuracy score: 0.864
mean recall score: 0.0
mean f1 score: 0.0
 
support vector machines {'kernel': 'rbf'}
mean accuracy score: 0.866
mean recall score: 0.01
mean f1 score: 0.02

Conclusion:
The random forest seems to be the best classifier, but the recall scores for the SVM look suspicious.
It's not normal for them to be 0.
The f1 score equals 0 because he is computed from the recall score.
It must be because the data is unbalanced (yes it is).

-------------Test n°3-------------
Test 10 fold cross validation with balanced data(random Oversample):

random forest {'criterion': 'gini'}
              precision    recall  f1-score   support

           0       0.94      0.96      0.95      1382
           1       0.70      0.60      0.65       217

    accuracy                           0.91      1599
   macro avg       0.82      0.78      0.80      1599
weighted avg       0.91      0.91      0.91      1599

random forest {'criterion': 'entropy'}
              precision    recall  f1-score   support

           0       0.94      0.96      0.95      2764
           1       0.70      0.61      0.66       434

    accuracy                           0.91      3198
   macro avg       0.82      0.79      0.80      3198
weighted avg       0.91      0.91      0.91      3198

k neighbors {'n_neighbors': 3}
              precision    recall  f1-score   support

           0       0.94      0.93      0.93      4146
           1       0.58      0.63      0.60       651

    accuracy                           0.89      4797
   macro avg       0.76      0.78      0.77      4797
weighted avg       0.89      0.89      0.89      4797

k neighbors {'n_neighbors': 5}
              precision    recall  f1-score   support

           0       0.94      0.90      0.92      5528
           1       0.50      0.65      0.56       868

    accuracy                           0.86      6396
   macro avg       0.72      0.77      0.74      6396
weighted avg       0.88      0.86      0.87      6396

support vector machines {'C': 0.1}
              precision    recall  f1-score   support

           0       0.94      0.81      0.87      6910
           1       0.36      0.68      0.47      1085

    accuracy                           0.79      7995
   macro avg       0.65      0.74      0.67      7995
weighted avg       0.86      0.79      0.82      7995

support vector machines {'C': 0.8}
              precision    recall  f1-score   support

           0       0.94      0.78      0.85      8292
           1       0.33      0.69      0.45      1302

    accuracy                           0.77      9594
   macro avg       0.64      0.74      0.65      9594
weighted avg       0.86      0.77      0.80      9594

support vector machines {'C': 0.9}
              precision    recall  f1-score   support

           0       0.94      0.76      0.84      9674
           1       0.32      0.70      0.44      1519

    accuracy                           0.76     11193
   macro avg       0.63      0.73      0.64     11193
weighted avg       0.86      0.76      0.79     11193

support vector machines {'C': 1}
              precision    recall  f1-score   support

           0       0.94      0.75      0.84     11056
           1       0.31      0.71      0.43      1736

    accuracy                           0.75     12792
   macro avg       0.63      0.73      0.64     12792
weighted avg       0.86      0.75      0.78     12792

support vector machines {'C': 1.1}
              precision    recall  f1-score   support

           0       0.94      0.74      0.83     12438
           1       0.31      0.72      0.43      1953

    accuracy                           0.74     14391
   macro avg       0.63      0.73      0.63     14391
weighted avg       0.86      0.74      0.78     14391

support vector machines {'C': 1.2}
              precision    recall  f1-score   support

           0       0.95      0.74      0.83     13820
           1       0.30      0.73      0.43      2170

    accuracy                           0.74     15990
   macro avg       0.63      0.73      0.63     15990
weighted avg       0.86      0.74      0.77     15990

support vector machines {'C': 1.3}
              precision    recall  f1-score   support

           0       0.95      0.73      0.83     15202
           1       0.30      0.74      0.43      2387

    accuracy                           0.73     17589
   macro avg       0.62      0.74      0.63     17589
weighted avg       0.86      0.73      0.77     17589

support vector machines {'C': 1.4}
              precision    recall  f1-score   support

           0       0.95      0.73      0.82     16584
           1       0.30      0.75      0.43      2604

    accuracy                           0.73     19188
   macro avg       0.62      0.74      0.63     19188
weighted avg       0.86      0.73      0.77     19188

support vector machines {'kernel': 'rbf'}
              precision    recall  f1-score   support

           0       0.95      0.72      0.82     17966
           1       0.30      0.75      0.43      2821

    accuracy                           0.73     20787
   macro avg       0.62      0.74      0.62     20787
weighted avg       0.86      0.73      0.77     20787

support vector machines {'kernel': 'linear'}
              precision    recall  f1-score   support

           0       0.95      0.73      0.82     19348
           1       0.30      0.75      0.43      3038

    accuracy                           0.73     22386
   macro avg       0.63      0.74      0.63     22386
weighted avg       0.86      0.73      0.77     22386

Conclusion:
Random forest still seems to be the best classifier.
But now the results are better for SVM (precision and recall different of 0 for class "good")
It means that now SVM can classify "good" wine, before it wasn't really possible. 


Attention: 
Ici on test avec différents paramètres mais se serait mieux de choisir le meilleur paramètres possible. 
C'est pour ça qu'on utilise Grid search CV. Parce que on pourrait avoir SVC(C = 1.2, gamma =  0.9, kernel= 'rbf') par exemple et nous on test que SVC(kernel = 'linear').
Donc test 4 va permettre de trouver le meilleur paramètre pour chaque algo avec ce dataset.

-------------Test n°4-------------
Hyperparameter tuning de Random Forest, SVC et K-Nearest Neighbors avec le dataset données.
The results are the best Hyperparameter for each algorithm:

Support Vector Machine {'C': 1.4, 'gamma': 1.3, 'kernel': 'rbf'}
Random Forest: {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 400, 'random_state': 18}
K-Nearest Neighbors: {'n_neighbors': 16}

-------------Test n°5-------------
Test avec :
	10 Fold Cross Validation
	le meilleur des hyperparametres
	unbalanced dataset

Random Forest {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 400, 'random_state': 18}
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      1382
           1       0.75      0.18      0.30       217

    accuracy                           0.88      1599
   macro avg       0.82      0.59      0.62      1599
weighted avg       0.87      0.88      0.85      1599

K-Nearest Neighbors {'n_neighbors': 16}
              precision    recall  f1-score   support

           0       0.88      0.99      0.93      2764
           1       0.68      0.12      0.20       434

    accuracy                           0.87      3198
   macro avg       0.78      0.55      0.56      3198
weighted avg       0.85      0.87      0.83      3198

Support Vector Machines {'C': 1.4, 'gamma': 1.3, 'kernel': 'rbf'}
              precision    recall  f1-score   support

           0       0.89      0.99      0.94      4146
           1       0.82      0.18      0.30       651

    accuracy                           0.88      4797
   macro avg       0.85      0.59      0.62      4797
weighted avg       0.88      0.88      0.85      4797

-------------Test n°6-------------
Test avec :
	10 Fold Cross Validation
	le meilleur des hyperparametres
	balanced dataset (random Oversample)

Random Forest {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 400, 'random_state': 18}
              precision    recall  f1-score   support

           0       0.97      0.81      0.88      1382
           1       0.40      0.82      0.54       217

    accuracy                           0.81      1599
   macro avg       0.68      0.81      0.71      1599
weighted avg       0.89      0.81      0.83      1599

K-Nearest Neighbors {'n_neighbors': 16}
              precision    recall  f1-score   support

           0       0.96      0.77      0.85      2764
           1       0.35      0.78      0.48       434

    accuracy                           0.77      3198
   macro avg       0.65      0.77      0.67      3198
weighted avg       0.87      0.77      0.80      3198

Support Vector Machines {'C': 1.4, 'gamma': 1.3, 'kernel': 'rbf'}
              precision    recall  f1-score   support

           0       0.94      0.84      0.89      4146
           1       0.39      0.63      0.48       651

    accuracy                           0.81      4797
   macro avg       0.66      0.74      0.68      4797
weighted avg       0.86      0.81      0.83      4797

Conclusion:
Le random forest reste le meilleur algo je pense mais ils se sont tous améliorés.

-------------Test n°7-------------
Test des baselines:
Most frequent algorithm:{'accuracy': 0.41, 'recall': 0.16666666666666666, 'precision': 0.06833333333333333, 'f1_score': 0.09692671394799053}
Uniform algorithm:{'accuracy': 0.16, 'recall': 0.16984363223809754, 'precision': 0.1567774452590194, 'f1_score': 0.11440071153626141}
Stratified algorithm{'accuracy': 0.37, 'recall': 0.15560957569634867, 'precision': 0.15749142972373462, 'f1_score': 0.1561924278579868}